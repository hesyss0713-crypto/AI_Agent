"git_repo":
  supervisor_message:
    command: "git"
    action: "clone_repo"
    target: "https://github.com/username/repo"
    metadata: null
  
"file_content":
  coder_message:
    command: "git"
    action: "repo_files"
    result: "success"
    metadata: 
      files: 
        - filename: "model.py"
          language: "python"
          content: |
            import torch
            import torch.nn as nn

            class SimpleMLP(nn.Module):
                def __init__(self, input_dim=784, hidden_dim=128, output_dim=10):
                    super(SimpleMLP, self).__init__()
                    self.layers = nn.Sequential(
                        nn.Linear(input_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Linear(hidden_dim, output_dim)
                    )
                
                def forward(self, x):
                    return self.layers(x)
        
        - filename: "train.py"
          language: "python"
          content: |
                  import torch
                  import torch.nn as nn
                  import torch.optim as optim
                  from torchvision import datasets, transforms
                  from torch.utils.data import DataLoader

                  from model import SimpleMLP

                  # 하이퍼파라미터
                  batch_size = 64
                  lr = 0.001
                  epochs = 5

                  # 데이터셋 (MNIST 예시)
                  transform = transforms.Compose([
                      transforms.ToTensor(),
                      transforms.Lambda(lambda x: x.view(-1))  # (1,28,28) -> (784,)
                  ])

                  train_dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
                  test_dataset = datasets.MNIST(root="./data", train=False, transform=transform, download=True)

                  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                  test_loader = DataLoader(test_dataset, batch_size=batch_size)

                  # 모델/손실/최적화기
                  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                  model = SimpleMLP().to(device)
                  criterion = nn.CrossEntropyLoss()
                  optimizer = optim.Adam(model.parameters(), lr=lr)

                  # 학습 루프
                  for epoch in range(epochs):
                      model.train()
                      total_loss = 0
                      for x, y in train_loader:
                          x, y = x.to(device), y.to(device)

                          optimizer.zero_grad()
                          preds = model(x)
                          loss = criterion(preds, y)
                          loss.backward()
                          optimizer.step()

                          total_loss += loss.item()

                      print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}")

                  # 평가
                  model.eval()
                  correct, total = 0, 0
                  with torch.no_grad():
                      for x, y in test_loader:
                          x, y = x.to(device), y.to(device)
                          preds = model(x)
                          predicted = preds.argmax(dim=1)
                          correct += (predicted == y).sum().item()
                          total += y.size(0)

                  print(f"Test Accuracy: {100*correct/total:.2f}%")
      